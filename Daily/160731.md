## Daily Review 2016. 7. 31


1. DataScience Notebooks

	Model Validation
	
	estimator has score method.
	
	knn.score(X_test, y_test)
	
	sklearn.cross_validation
	
	- cross_val_score(estimator, X, y, cv=k)
	
	sklearn.preprocessing
	
	- PolynomialFeatures: Generate polynomial and interaction features. ex) [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].
	
	sklearn.pipeline
	
	- make_pipeline: Construct a Pipeline from the given estimators.
	- Pipeline: Pipeline of transforms with a final estimator.  Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.  
The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’, as in the example below.****

	sklearn.learning_curve
	
	- validation_curve: Validation curve.  
Determine training and test scores for varying parameter values.  
Compute scores for an estimator with different values of a specified parameter. This is similar to grid search with one parameter. However, this will also compute training scores and is merely a utility for plotting the results.
	- learning_curve: Learning curve.  
Determines cross-validated training and test scores for different training set sizes.  
A cross-validation generator splits the whole dataset k times in training and test data. Subsets of the training set with varying sizes will be used to train the estimator and a score for each training subset size and the test set will be computed. Afterwards, the scores will be averaged over all k runs for each training subset size.
	
2. [DEEP LEARNING FOR CHATBOTS, PART 1](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/)

	INTRODUCTION
	
	챗봇이 대세가 되고 있다..!  
	
	**A TAXONOMY OF MODELS**
	1. Retrieval-based models (easier) : 저장소에 저장되어 있는 응답, 문서에 따라서 가장 높은 점수의 답변을 반환. 실용적인 측면으로 많이 사용.
	2. Generative models(harder) : 질문에 따라서 알맞은 답을 생성해서 반환. 잘 반환해주면 똑똑해보이나.. 어렵다.
	
	Deep Learning은 둘다 포함하고 있지만, 2번 Generatvie model로 더 집중하고 있음.
	
	ex) [Sequence to Sequence Architecutre](http://arxiv.org/abs/1409.3215)
	
	그 밖에 대화에 관한 문제들.
	
	- 	Long vs Short Conversations
	-  Open Domain vs Closed domain
	-  Incorporating context
	-  Coherent personality
	-  Evalution of models
	-  Intention and diversity
		
3. [DEEP LEARNING FOR CHATBOTS, PART 2](http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/)

	IMPLEMENTING A RETRIEVAL-BASED MODEL IN TENSORFLOW
	
	Retrieval-based Bots.
	
	Generative model이 잘 작동하지 않은 예)
	[Google's Smart Reply](http://arxiv.org/abs/1606.04870)
	
	우리가 사용할 데이터
	
	The Ubuntu dialog corpus([paper](http://arxiv.org/abs/1506.08909), [github](https://github.com/rkadlec/ubuntu-ranking-dataset-creator))
	
	이미 pre-processing 되어 있는 데이들이고, pandas를 통해서 확인.
	
	-------------------------
	
	Baselines
	
	Evalutaion: recall@k metric
	
	시도
	
	- random prediction
	- tf-dif
	- dual enconder LSTM([paper](http://arxiv.org/abs/1506.08909))
		- [GloVe](http://nlp.stanford.edu/projects/glove/) 사용
		- 자세한 동작원리는.. 논문을 봅시다

	---------------------------
	
	Evaluating the Model
	
	recall@5 = 0.92 정도의 성능.
	
	코드, 논문을 보고 이해하고 구현해보자!!
	
4. CS231n

	####Image Classification pipeline
	
	Image = 3D arrays of numbers(0~255)
	
	Challenges:
	
	- Illumination
	- Deformation
	- Occlusion
	- Background clutter
	- Intraclas variation
	
	-----------------
	
	**Data-driven approach!**
	
	First classifier: Nearest Neighbor CLassifier
	
	hyperparameters
	
	- L1-distance / L2-distance
	- k
	
	Aside: Approximate Nearest Neighbor
	
	k-NN on images never userd.
	
	------------------
	
	**Cross-validation**  
	cycle through the choice of which fold is the validation fold, average results.
	
	
	#### Linear Classification
	
	Parametric approach.
	
	f(x, W) = Wx + b  
	W: weight , b: bias
	
	score function